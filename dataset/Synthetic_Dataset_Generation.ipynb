{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YccoYgyXWQ9K",
        "hn18Eg7meGVS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing : Synthetic Dataset Generation"
      ],
      "metadata": {
        "id": "PJQncwkxe7Pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ],
      "metadata": {
        "id": "YccoYgyXWQ9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVJS_VipIr1l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "o-b3aoFeBLW4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raingo/TGIF-Release/"
      ],
      "metadata": {
        "id": "bkpIImniBNlL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/TGIF-Release/data/tgif-v1.0.tsv', sep = '\\t', names =['url','desc'])"
      ],
      "metadata": {
        "id": "uqaDVLN0BMl8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jhHB5q1zBPC8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Question-Answer pair generation\n",
        "\n",
        "We employ a rather specialised version of the T5 transformer that has been finetuned on the *question generation* task, which makes things a bit easier, since it is easier to extract a variety of potential answers from the text descriptions, and the question generation pipeline takes care of the rest.\n",
        "\n",
        "\n",
        "We define a variety of answers such as:\n",
        "\n",
        "1. living entities (for entity recognition)\n",
        "2. nouns (for object recognitions)\n",
        "3. verb (action recognition)\n",
        "4. prepositional phrases (related to object detection capabilities)\n",
        "5. colors (descriptions of objects)"
      ],
      "metadata": {
        "id": "qkpim_3lgVbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def getcolors():\n",
        "    html = urllib.request.urlopen('http://www.w3schools.com/colors/colors_names.asp').read()\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    children = [item.findChildren() for item in soup.find_all('tr')]\n",
        "    colors = [''.join( ' '+x if 'A' <= x <= 'Z' else x for x in item[0].text.replace(u'\\xa0', '')).strip().lower() for item in children]\n",
        "    return colors[1:]"
      ],
      "metadata": {
        "id": "ETqANP5AigP1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load question generation model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
      ],
      "metadata": {
        "id": "VTt4vPUBeCes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_question(answer, context, max_length=64):\n",
        "    input_text = f\"answer: {answer}  context: {context} </s>\"\n",
        "    features = tokenizer([input_text], return_tensors='pt')\n",
        "\n",
        "    output = model.generate(input_ids=features['input_ids'],\n",
        "                   attention_mask=features['attention_mask'],\n",
        "                   max_length=max_length)\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def extract_potential_answers(description):\n",
        "    doc = nlp(description)\n",
        "    potential_answers = []\n",
        "\n",
        "    potential_answers.extend([ent.text for ent in doc.ents])\n",
        "    potential_answers.extend([chunk.text for chunk in doc.noun_chunks])\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"VERB\":\n",
        "            potential_answers.append(token.text)\n",
        "            if token.dep_ == \"ROOT\" and token.right_edge.i < len(doc) - 1:\n",
        "                potential_answers.append(doc[token.i:token.right_edge.i + 1].text)\n",
        "\n",
        "    adj_noun_pairs = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"ADJ\":\n",
        "            for child in token.children:\n",
        "                if child.pos_ == \"NOUN\":\n",
        "                    adj_noun_pairs.append(f\"{token.text} {child.text}\")\n",
        "    potential_answers.extend(adj_noun_pairs)\n",
        "\n",
        "    prep_phrases = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"ADP\" and token.dep_ == \"prep\":\n",
        "            phrase = [token.text]\n",
        "            for child in token.subtree:\n",
        "                if child != token:\n",
        "                    phrase.append(child.text)\n",
        "            prep_phrases.append(\" \".join(phrase))\n",
        "    potential_answers.extend(prep_phrases)\n",
        "\n",
        "    colors = getcolors()\n",
        "    color_phrases = [token.sent.text for token in doc if token.text.lower() in colors]\n",
        "    potential_answers.extend(color_phrases)\n",
        "\n",
        "    return list(set(potential_answers))\n",
        "\n",
        "def generate_qa_pairs(description):\n",
        "    qa_pairs = []\n",
        "    potential_answers = extract_potential_answers(description)\n",
        "\n",
        "    sampled_answers = random.sample(potential_answers, min(5, len(potential_answers)))\n",
        "    for answer in sampled_answers:\n",
        "        question = get_question(answer, description)\n",
        "        qa_pairs.append((question, answer))\n",
        "\n",
        "    # summary based questions\n",
        "    fixed_questions = [\n",
        "        \"What is happening in the image?\",\n",
        "        \"Can you describe the scene?\",\n",
        "        \"What's the main focus of this description?\",\n",
        "        \"Who are the main characters or objects in this scene?\",\n",
        "        \"What's the most striking feature of this description?\"\n",
        "    ]\n",
        "    for q in random.sample(fixed_questions, 2): #random sampling for diversity\n",
        "        qa_pairs.append((q, description))\n",
        "\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "yia1z1lPy81o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "all_qa_triplets = []\n",
        "urls = []\n",
        "tag = 1\n",
        "\n",
        "for i in tqdm(range(df.shape[0])):\n",
        "    description = df.loc[i, 'desc']\n",
        "    url = df.loc[i, 'url'] if 'url' in df.columns else None\n",
        "    urls.append(url)\n",
        "\n",
        "    qa_pairs = generate_qa_pairs(description)\n",
        "    for question, answer in qa_pairs:\n",
        "        all_qa_triplets.append({'question': question, 'answer': answer, 'url': url})\n",
        "\n",
        "\n",
        "    # saving checkpoints once every 2500 qa pairs are generated\n",
        "    while len(all_qa_triplets) >= 2500:\n",
        "        checkpoint = {\n",
        "            'qa_triplets': all_qa_triplets[:2500]\n",
        "        }\n",
        "        with open(f'checkpoint_{tag}.json', 'w') as f:\n",
        "            json.dump(checkpoint, f)\n",
        "        print(f\"Checkpoint saved: checkpoint_{tag}.json\")\n",
        "        shutil.copy(f\"/content/checkpoint_{tag}.json\", \"/content/drive/MyDrive\")\n",
        "\n",
        "        all_qa_triplets = all_qa_triplets[2500:]\n",
        "        tag+=1\n",
        "\n",
        "if all_qa_triplets:\n",
        "    checkpoint = {\n",
        "        'qa_triplets': all_qa_triplets\n",
        "    }\n",
        "    with open('checkpoint_final.json', 'w') as f:\n",
        "        json.dump(checkpoint, f)\n",
        "    print(\"Final checkpoint saved: checkpoint_final.json\")"
      ],
      "metadata": {
        "id": "29qgdluB0DhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZupJ01Ebs6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Base and Subject-Predicate-Object triplets\n",
        "\n",
        "Generating Subject-Predicate-Object triplets from the natural language descriptions, can train our model better with respect to action recognition tasks, which is one of the toughest parts involved in VideoQA. Owing to time constraints, we were unable to generate reasonable triplets."
      ],
      "metadata": {
        "id": "hn18Eg7meGVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# taken from https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text"
      ],
      "metadata": {
        "id": "JtHWS_dmYJPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import math\n",
        "import torch"
      ],
      "metadata": {
        "id": "oSo9r51NZzcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
      ],
      "metadata": {
        "id": "HWnovp7gZzuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://huggingface.co/Babelscape/rebel-large\n",
        "def extract_relations_from_model_output(text):\n",
        "    relations = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "    for token in text_replaced.split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        relations.append({\n",
        "            'head': subject.strip(),\n",
        "            'type': relation.strip(),\n",
        "            'tail': object_.strip()\n",
        "        })\n",
        "    return relations"
      ],
      "metadata": {
        "id": "iz3ib_eUZ3sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.relations = []\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    def add_relation(self, r):\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")"
      ],
      "metadata": {
        "id": "cBQ3qXh4Z9tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a knowledge base from text\n",
        "def from_small_text_to_kb(text, verbose=False):\n",
        "    kb = KB()\n",
        "\n",
        "    # Tokenizer text\n",
        "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
        "                            return_tensors='pt')\n",
        "    if verbose:\n",
        "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
        "\n",
        "    # Generate\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 216,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": 3\n",
        "    }\n",
        "    generated_tokens = model.generate(\n",
        "        **model_inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    for sentence_pred in decoded_preds:\n",
        "        relations = extract_relations_from_model_output(sentence_pred)\n",
        "        for r in relations:\n",
        "            kb.add_relation(r)\n",
        "\n",
        "    return kb"
      ],
      "metadata": {
        "id": "9C4xbV8aaBCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"A little kitten is playing with a dog on the floor in the house\"\n",
        "\n",
        "kb = from_small_text_to_kb(text, verbose=True)\n",
        "kb.print()"
      ],
      "metadata": {
        "id": "bfEuV_HJaI2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trying textacy"
      ],
      "metadata": {
        "id": "fbDFEfOcaMOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy"
      ],
      "metadata": {
        "id": "Z1umY7ZzawK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import textacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = nlp(u'A group of men are dancing')\n",
        "\n",
        "text_ext = textacy.extract.subject_verb_object_triples(text)"
      ],
      "metadata": {
        "id": "rR0RAbB5crWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(text_ext)"
      ],
      "metadata": {
        "id": "hxrpaXdxdJ5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jwsmn8bevJq0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}