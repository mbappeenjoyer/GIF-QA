{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T14:26:22.746362Z","iopub.execute_input":"2024-09-26T14:26:22.746704Z","iopub.status.idle":"2024-09-26T14:26:22.751285Z","shell.execute_reply.started":"2024-09-26T14:26:22.746670Z","shell.execute_reply":"2024-09-26T14:26:22.750436Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False","metadata":{"execution":{"iopub.status.busy":"2024-09-26T14:26:22.809340Z","iopub.execute_input":"2024-09-26T14:26:22.809658Z","iopub.status.idle":"2024-09-26T14:26:22.813458Z","shell.execute_reply.started":"2024-09-26T14:26:22.809626Z","shell.execute_reply":"2024-09-26T14:26:22.812534Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/raingo/TGIF-Release/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/working/TGIF-Release/data/tgif-v1.0.tsv', sep = '\\t', names=['urls','description'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def genprompt(inp: str):\n    p1 = f\"\"\"\n    You are a highly qualified expert trained to generate questions based on input text.\n    Your task is to analyze the TEXT below and generate ONE question along with its answer that involves the entities, objects, or scenery described in the text.\n    The question should strictly focus on the information given in the TEXT. Do not speculate beyond the provided details or context. Your question can be simple or analytical, but it must be directly related to the entities or descriptions found in the TEXT.\n    Provide both the question and answer in the following format:\n    Question: <your question>\n    Answer: <your answer>\n\n    Examples:\n\n    Text: The farmer's market was filled with fresh strawberries, apples, and oranges.\n    Question: What fruits were mentioned in the description of the farmer's market?\n    Answer: Strawberries, apples, and oranges.\n\n    Text: An old lighthouse stood at the edge of the rocky coastline.\n    Question: What structure is mentioned in the text?\n    Answer: The structure mentioned in the text is an old lighthouse.\n\n    Text: The bustling city square was adorned with colorful banners and food stalls for the annual festival.\n    Question: What event is taking place in the city square?\n    Answer: The annual festival is taking place in the city square.\n\n    Now, analyze the following TEXT and generate your question and answer:\n\n    TEXT: {inp}\n\n    Question:\n    Answer:\n    \"\"\"\n    return p1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_cpp import Llama\n\nllm = Llama(\n  model_path=\"/kaggle/working/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", \n  n_ctx=2048,  \n  n_threads=8,            \n  n_gpu_layers=-1\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-26T14:26:22.931208Z","iopub.execute_input":"2024-09-26T14:26:22.931829Z","iopub.status.idle":"2024-09-26T14:26:22.935582Z","shell.execute_reply.started":"2024-09-26T14:26:22.931795Z","shell.execute_reply":"2024-09-26T14:26:22.934591Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfor i in tqdm(range(len(df))):\n    prompt = genprompt(df['statement'][i], df['sample_input'][i], df['sample_output'][i])\n    try:\n        output = llm(\n            f'[INST] {prompt} [/INST]',  # Prompt\n            max_tokens=1024,  # Generate up to 1024 tokens\n            stop=[\"</s>\"],   # Example stop token\n            echo=False,       # Whether to echo the prompt\n            top_p = 1,\n            top_k = 1,\n            temperature=0,\n        )\n        result.append({\n          'name' : df['name'][i],\n          'year' : df['year'][i],\n          'round' : df['round'][i],\n          'input' : df['input'][i],\n          'output' : df['output'][i],\n          'solution' : df['solution'][i],\n          'code_ACTUAL' : df['code'][i],\n          'statement': df['statement'][i],\n          'sample_input': df['sample_input'][i],\n          'sample_output': df['sample_output'][i],\n          'model_output': output,\n      })\n\n    except Exception as e:\n        print(\"weird case \")\n        bad.append(i)\n    if (i + 1) % 50 == 0:\n        checkpoint_data = {\n            'result': result,\n            'bad': bad,\n        }\n        with open(f'checkpoint_{i + 1}.json', 'w') as f:\n            json.dump(checkpoint_data, f, indent=4)\n        print(f\"Checkpoint saved at iteration {i + 1}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}