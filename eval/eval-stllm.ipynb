{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://arxiv.org/pdf/2404.00308"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install evaluate\n","!pip install faiss-gpu\n","!python -m pip install scikit-image\n","!pip install pillow\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install peft\n","!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from transformers import CLIPImageProcessor, GPT2Tokenizer, CLIPProcessor, CLIPModel, GPT2LMHeadModel, Blip2QFormerConfig, Blip2QFormerModel\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from nltk.translate.bleu_score import sentence_bleu\n","from evaluate import load\n","import collections\n","from torch.cuda.amp import autocast\n","import numpy as np\n","from nltk.tokenize import TreebankWordTokenizer\n","from peft import LoraConfig, get_peft_model, TaskType"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('/kaggle/input/yesr/pytorch/default/1')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from retriever import GIFFrameRetriever\n","clip_cut = GIFFrameRetriever()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import clip\n","clipm, clipp = clip.load(\"ViT-B/32\", device=\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GQADataset(torch.utils.data.Dataset):\n","    def __init__(self, final_data, clip_cut, nf, clipm, clipp, train_setting = True):\n","        self.questions = final_data['question']\n","        self.image_urls = final_data['url']\n","        self.answers = final_data['answer']\n","        self.clip_cut= clip_cut\n","        self.clipm = clipm\n","        self.clipp = clipp\n","        self.processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","        self.train_setting = train_setting\n","        self.nf = nf\n","\n","    def __len__(self):\n","        return len(self.image_urls)\n","\n","    def __getitem__(self, idx):\n","\n","        url = self.image_urls[idx]\n","        keyframes = self.clip_cut.retrieve_images_from_gif(url, self.clipm, self.clipp, self.questions[idx], nf)\n","\n","        questions = self.questions[idx]\n","        answers = self.answers[idx]\n","\n","        processed_keyframes = [self.processor(frame, return_tensors='pt')['pixel_values'].squeeze(0) for frame in keyframes]\n","        processed_keyframes = torch.stack(processed_keyframes)\n","\n","        tokens, mask, c_len, t_len = self.pad_sequences(self.questions[idx], self.answers[idx], nf)\n","\n","        tokens = tokens.long()\n","        mask = mask.long()\n","\n","        sample = {'images' : processed_keyframes,\n","                  'tokens': tokens,\n","                  'mask': mask,\n","                  'c_len' : c_len,\n","                  't_len' : t_len,\n","                  'answers' : answers,\n","                  'questions' : questions}\n","\n","        return sample\n","\n","    def pad_sequences(self, questions, answers, nf):\n","        m = [\n","          torch.tensor(self.tokenizer.encode('question: ')),\n","          torch.tensor(self.tokenizer.encode(' context:')),\n","          torch.tensor(self.tokenizer.encode('answer ')),\n","          torch.tensor(self.tokenizer.encode('<|endoftext|>')),\n","      ]\n","        m_mask = [\n","          torch.ones(len(m[0])),\n","          torch.ones(len(m[1])),\n","          torch.ones(len(m[2])),\n","          torch.zeros(len(m[3]))\n","      ]\n","\n","        if self.train_setting:\n","            q = torch.tensor(self.tokenizer.encode(str(questions)))\n","            a = torch.tensor(self.tokenizer.encode(str(answers)))\n","\n","            q, q_mask, leftover_tokens = self.make_padding(32, q, question=True)\n","\n","            c_len =  m[1].size(0)\n","            t_len = m[0].size(0) + nf + q.size(0) + m[1].size(0)\n","\n","            a, a_mask, _ = self.make_padding(32, a, leftover_tokens=leftover_tokens)\n","\n","            if len((a == 0).nonzero()) != 0:\n","                pad_start = (a == 0).nonzero()[0]\n","            else:\n","                pad_start = []\n","\n","            a = torch.cat((a, m[3])) if len(pad_start) == 0 else torch.cat((a[:pad_start], m[3], a[pad_start:]))\n","            q = torch.cat((m[1], torch.ones(nf), m[0],  q, m[2], a))\n","\n","            q_mask = torch.cat((m_mask[1], torch.ones(nf), m_mask[0],  q_mask, m_mask[2], a_mask, m_mask[3]))\n","\n","            return q, q_mask, c_len, t_len\n","        else:\n","            q = torch.tensor(self.tokenizer.encode(str(questions)))\n","            q, q_mask, _ = self.make_padding_test_setting(32, q)\n","\n","            c_len =  m[1].size(0)\n","            t_len = m[0].size(0) + nf + q.size(0) + m[1].size(0)\n","\n","            q = torch.cat((m[1], torch.ones(nf), m[0],  q, m[2]))\n","\n","            q_mask = torch.cat((m_mask[1], torch.ones(nf), m_mask[0],  q_mask, m_mask[2]))\n","            return q, q_mask, c_len, t_len\n","\n","\n","    def make_padding(self, max_len, tokens, question=False, leftover_tokens=0):\n","        padding = max_len - tokens.size(0)\n","        if padding > 0:\n","            if question:\n","                leftover_tokens = padding\n","                mask = torch.ones(tokens.size(0))\n","            else:\n","                tokens = torch.cat((tokens, torch.zeros(padding + leftover_tokens)))\n","                mask = torch.zeros(max_len + leftover_tokens)\n","\n","        elif padding == 0:\n","            if question:\n","                mask = torch.ones(tokens.size(0))\n","            else:\n","                mask = torch.zeros(tokens.size(0) + leftover_tokens)\n","                tokens = torch.cat((tokens, torch.zeros(leftover_tokens)))\n","\n","        elif padding < 0:\n","            if question:\n","                tokens = tokens[:max_len]\n","                mask = torch.ones(max_len)\n","            else:\n","                tokens = torch.cat((tokens[:max_len], torch.zeros(leftover_tokens)))\n","                mask = torch.zeros(max_len + leftover_tokens)\n","\n","        return tokens, mask, leftover_tokens\n","\n","\n","    def make_padding_test_setting(self, max_len, tokens, do_padding=False):\n","        padding = max_len - tokens.size(0)\n","        padding_len = 0\n","        if padding > 0:\n","            if do_padding:\n","                mask = torch.cat((torch.ones(tokens.size(0)), torch.zeros(padding)))\n","                tokens = torch.cat((tokens, torch.zeros(padding)))\n","                padding_len = padding\n","            else:\n","                mask = torch.ones(tokens.size(0))\n","        elif padding == 0:\n","            mask = torch.ones(max_len)\n","        elif padding < 0:\n","            tokens = tokens[:max_len]\n","            mask = torch.ones(max_len)\n","        return tokens, mask, padding_len"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["peft_config = LoraConfig(\n","  task_type=TaskType.CAUSAL_LM, inference_mode=False,\n","  r=8,\n","  lora_alpha=32, lora_dropout=0.1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, sizes):\n","        super().__init__()\n","        layers = []\n","        for i in range(len(sizes) - 1):\n","            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n","            if i < len(sizes) - 2:\n","                layers.append(nn.ReLU())\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GQAModel(nn.Module):\n","    def __init__(self, peft_config, nf, num_query_token=3, cross_attention_freq=2):\n","        super().__init__()\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","        for param in self.clip.parameters():\n","            param.requires_grad = False\n","\n","        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        self.llm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","        self.peft_config = peft_config\n","        self.llm = get_peft_model(self.llm, self.peft_config)\n","        self.nf=  nf\n","\n","        self.clip_vision_output_dim = self.clip.config.vision_config.hidden_size\n","\n","        self.mapper = MLP(sizes=(self.clip_vision_output_dim, 768, 768))\n","        self.dropout = nn.Dropout(0.1)\n","        self.Qformer, self.query_tokens = self.init_Qformer(nf)\n","        for param in self.Qformer.parameters():\n","            param.requires_grad = False\n","\n","        self.llm_proj = nn.Linear(self.Qformer.config.hidden_size, self.llm.config.hidden_size)\n","        self.image_proj = nn.Linear(512,self.Qformer.config.encoder_hidden_size)\n","\n","    def init_Qformer(self, num_query_token=3):\n","        qformer_config = Blip2QFormerConfig.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n","        qformer_config.query_length = num_query_token\n","\n","        Qformer = Blip2QFormerModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\", config=qformer_config)\n","        query_tokens = nn.Parameter(torch.zeros(1, num_query_token, qformer_config.hidden_size))\n","        query_tokens.data.normal_(mean=0.0, std=qformer_config.initializer_range)\n","\n","        return Qformer, query_tokens\n","\n","    def forward(self, images, tokens, mask, c_len):\n","        batch_size, num_frames, c, h, w = images.size()\n","\n","        images_reshaped = images.view(-1, c, h, w)\n","        image_features = self.clip.get_image_features(images_reshaped)\n","\n","        image_features = self.image_proj(image_features)\n","        image_features = image_features.view(batch_size, num_frames, -1)\n","\n","        image_atts = torch.ones(image_features.size()[:-1], dtype=torch.long).to(images.device)\n","        query_tokens = self.query_tokens.expand(batch_size, -1, -1) \n","        query_output = self.Qformer(\n","            query_embeds=query_tokens,\n","            encoder_hidden_states=image_features,\n","            encoder_attention_mask=image_atts,\n","            return_dict=True,\n","        )\n","\n","        V = self.llm_proj(query_output.last_hidden_state)\n","\n","        sigma = 0.1\n","        rho = 0.5 + sigma * torch.randn(1).item()\n","        rho = torch.clamp(torch.tensor(rho), 0.3, 0.7).item()\n","        dynamic_mask = torch.rand((V.shape[0], V.shape[1], V.shape[2]), device=V.device) > rho\n","\n","        V_masked = V * dynamic_mask.float()\n","        caption_features = self.llm.transformer.wte(tokens)\n","\n","        caption_features_masked = caption_features\n","        caption_features_unmasked = caption_features\n","\n","        for b in range(caption_features.shape[0]):\n","            caption_features_masked[b,c_len[b]:c_len[b]+self.nf,:] = V_masked[b]\n","        for b in range(caption_features.shape[0]):\n","            caption_features_masked[b,c_len[b]:c_len[b]+self.nf,:] = V[b]\n","\n","        llm_output_unmasked = self.llm(inputs_embeds=caption_features_unmasked, attention_mask=mask,output_hidden_states=True)\n","        llm_output_masked = self.llm(inputs_embeds=caption_features_masked, attention_mask=mask, output_hidden_states=True)\n","\n","        masked_out = llm_output_masked.hidden_states[-1]\n","        unmasked_out = llm_output_unmasked.hidden_states[-1]\n","\n","        return llm_output_masked, masked_out, unmasked_out, dynamic_mask\n","\n","    def generate(self, images, tokens, mask, c_len):\n","\n","\n","        batch_size, num_frames, c, h, w = images.size()\n","\n","        images_reshaped = images.view(-1, c, h, w)\n","        image_features = self.clip.get_image_features(images_reshaped)\n","\n","        image_features = self.image_proj(image_features)\n","        image_features = image_features.view(batch_size, num_frames, -1)\n","\n","        image_atts = torch.ones(image_features.size()[:-1], dtype=torch.long).to(images.device)\n","        query_tokens = self.query_tokens.expand(batch_size, -1, -1)\n","\n","        query_output = self.Qformer(\n","            query_embeds=query_tokens,\n","            encoder_hidden_states=image_features,\n","            encoder_attention_mask=image_atts,\n","            return_dict=True,\n","        )\n","\n","        V = self.llm_proj(query_output.last_hidden_state)\n","\n","        embedding = self.llm.transformer.wte(tokens)\n","\n","        for b in range(embedding.shape[0]):\n","                 embedding[b,c_len[b]:c_len[b]+self.nf,:] = V[b]\n","\n","        return embedding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/dataset/data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def treebank_tokenize(s):\n","    return TreebankWordTokenizer().tokenize(s)\n","def generate_beam(model, tokenizer,beam_size= 1,generated=None, entry_length=50, temperature=0.8, stop_token: str = \"<|endoftext|>\",):\n","    model.eval()\n","    stop_token_index = tokenizer.encode(stop_token)[0]\n","    tokens = None\n","    scores = None\n","    device = next(model.parameters()).device\n","    seq_lengths = torch.ones(beam_size, device=device)\n","    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n","    with torch.no_grad():\n","        for i in range(entry_length):\n","            outputs = model.llm(inputs_embeds=generated)\n","            logits = outputs.logits\n","\n","            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","\n","            logits = logits.softmax(-1).log()\n","            # final_logit\n","\n","            if scores is None:\n","                scores, next_tokens = logits.topk(beam_size, -1)\n","                generated = generated.expand(beam_size, *generated.shape[1:])\n","                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n","                if tokens is None:\n","                    tokens = next_tokens\n","                else:\n","                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n","                    tokens = torch.cat((tokens, next_tokens), dim=1)\n","            else:\n","                logits[is_stopped] = -float(np.inf)\n","                logits[is_stopped, 0] = 0\n","                scores_sum = scores[:, None] + logits\n","                seq_lengths[~is_stopped] += 1\n","                scores_sum_average = scores_sum / seq_lengths[:, None]\n","                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n","                    beam_size, -1\n","                )\n","                next_tokens_source = next_tokens // scores_sum.shape[1]\n","                seq_lengths = seq_lengths[next_tokens_source]\n","                next_tokens = next_tokens % scores_sum.shape[1]\n","                next_tokens = next_tokens.unsqueeze(1)\n","                tokens = tokens[next_tokens_source]\n","                tokens = torch.cat((tokens, next_tokens), dim=1)\n","                generated = generated[next_tokens_source]\n","                scores = scores_sum_average * seq_lengths\n","                is_stopped = is_stopped[next_tokens_source]\n","\n","\n","            next_token_embed = model.llm.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n","\n","            generated = torch.cat((generated, next_token_embed), dim=1)\n","            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n","            if is_stopped.all():\n","                break\n","    scores = scores / seq_lengths\n","    output_list = tokens.cpu().numpy()\n","    output_texts = [\n","        tokenizer.decode(output[: int(length)])\n","        for output, length in zip(output_list, seq_lengths)\n","    ]\n","    order = scores.argsort(descending=True)\n","    output_texts = [output_texts[i] for i in order]\n","    return output_texts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate(model, dataloader):\n","    model.eval()\n","    model= model.to(device)\n","    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","    bleu_avg1=0.\n","    f1_avg = 0.\n","    acc = 0.\n","    acc_oe = 0.\n","    acc_yn = 0.\n","    c_oe =1e-9\n","    c_yn =1e-9\n","\n","    for batch in tqdm(dataloader, desc=\"Testing\"):\n","        images, tokens, mask, c_len , t_len = batch['images'], batch['tokens'], batch['mask'], batch['c_len'], batch['t_len']\n","        images = images.to(device)\n","        tokens = tokens.to(device)\n","        mask = mask.to(device)\n","        c_len = c_len.to(device)\n","\n","        with autocast(dtype=torch.float16):\n","            with torch.no_grad():\n","                embed = model.generate(images, tokens, mask, c_len)\n","                out_text = generate_beam(model, model.tokenizer,generated=embed,entry_length=30, temperature=1)[0]\n","\n","        out_text = out_text.split(\"<|endoftext|>\")[0]\n","\n","        print('Question: ', batch['questions'])\n","        print('Answer: ', batch['answers'])\n","        print(\"Generated_Answer: \", out_text)\n","\n","        if out_text.lower()==(batch['answers'][0]).lower():\n","            acc+=1\n","\n","        reference = [str((batch['answers'][0]).lower())]\n","        candidate = [out_text]\n","\n","        bleu_1 = sentence_bleu(reference[0], candidate[0], weights=(1, 0, 0, 0))\n","        f1_avg += compute_f1(tokenizer.encode(reference[0]),tokenizer.encode(candidate[0]))\n","        bleu_avg1+=bleu_1\n","\n","    # print('------------')\n","    print(\"BLEU {}\".format(round(bleu_avg1/len(dataloader),3)))\n","    print(\"F1 {}\".format(round(f1_avg/len(dataloader),3)))\n","    print(\"Accuracy {}\".format(round(acc/len(dataloader),3)))\n","\n","\n","def compute_f1(gold_toks, pred_toks):\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        return int(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nf=5\n","ckpt = torch.load('/kaggle/input/weights/pytorch/default/1/checkpoint_2.pt')\n","model = GQAModel(peft_config, nf).to(device)\n","model.load_state_dict(ckpt['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data = df.sample(frac=0.007).reset_index()\n","test_dataset = GQADataset(test_data,  clip_cut, nf, clipm, clipp, train_setting = False)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate(model, test_loader)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
